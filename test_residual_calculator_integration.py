#!/usr/bin/env python3
"""
ì”ì°¨ ê³„ì‚°ê¸° í†µí•© í…ŒìŠ¤íŠ¸

ì‹¤ì œ ì•”í˜¸í™”í ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì”ì°¨ ê³„ì‚° ì‹œìŠ¤í…œì˜ 
ì „ì²´ ì›Œí¬í”Œë¡œìš°ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import pandas as pd
import numpy as np
import logging
from pathlib import Path
import time
import warnings
from datetime import datetime, timedelta

from crypto_dlsa_bot.ml.residual_calculator import (
    ResidualCalculator, ResidualCalculatorConfig, ResidualQualityMetrics
)
from crypto_dlsa_bot.ml.crypto_ipca_model import CryptoIPCAModel
from crypto_dlsa_bot.ml.ipca_preprocessor import IPCADataPreprocessor, IPCAPreprocessorConfig

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings('ignore', category=RuntimeWarning)
warnings.filterwarnings('ignore', category=UserWarning)


def load_processed_data():
    """ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
    data_dir = Path("data/processed/ohlcv")
    
    # ê°€ì¥ ìµœê·¼ ë°ì´í„° íŒŒì¼ ì°¾ê¸°
    parquet_files = []
    for timeframe in ['1d', '1h', '4h']:
        timeframe_dir = data_dir / timeframe / 'multi_symbol'
        if timeframe_dir.exists():
            files = list(timeframe_dir.glob('*.parquet'))
            if files:
                latest_file = max(files, key=lambda x: x.stat().st_mtime)
                parquet_files.append((timeframe, latest_file))
    
    if not parquet_files:
        raise FileNotFoundError("ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # 1ì¼ ë°ì´í„° ìš°ì„  ì‚¬ìš©
    for timeframe, file_path in parquet_files:
        if timeframe == '1d':
            logger.info(f"1ì¼ ë°ì´í„° ë¡œë“œ: {file_path}")
            return pd.read_parquet(file_path)
    
    # 1ì¼ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ íŒŒì¼ ì‚¬ìš©
    timeframe, file_path = parquet_files[0]
    logger.info(f"{timeframe} ë°ì´í„° ë¡œë“œ: {file_path}")
    return pd.read_parquet(file_path)


def prepare_crypto_data(df, max_symbols=8):
    """ì•”í˜¸í™”í ë°ì´í„° ì¤€ë¹„"""
    logger.info("ì•”í˜¸í™”í ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
    
    # í•„ìš”í•œ ì»¬ëŸ¼ í™•ì¸
    required_cols = ['timestamp', 'symbol', 'close', 'volume']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"í•„ìˆ˜ ì»¬ëŸ¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤: {missing_cols}")
    
    # ë°ì´í„° ì •ë¦¬
    result_df = df.copy()
    
    # timestampë¥¼ dateë¡œ ë³€í™˜
    if 'date' not in result_df.columns:
        result_df['date'] = pd.to_datetime(result_df['timestamp'])
    
    # ìˆ˜ìµë¥  ê³„ì‚°
    if 'return' not in result_df.columns:
        result_df = result_df.sort_values(['symbol', 'date'])
        result_df['return'] = result_df.groupby('symbol')['close'].pct_change()
    
    # NaN ì œê±°
    result_df = result_df.dropna(subset=['return'])
    
    # ë°ì´í„° í•„í„°ë§ (ì¶©ë¶„í•œ ê´€ì¸¡ì¹˜ê°€ ìˆëŠ” ì‹¬ë³¼ë§Œ)
    symbol_counts = result_df.groupby('symbol').size()
    valid_symbols = symbol_counts[symbol_counts >= 100].index  # ìµœì†Œ 100ê°œ ê´€ì¸¡ì¹˜
    result_df = result_df[result_df['symbol'].isin(valid_symbols)]
    
    # ìƒìœ„ Nê°œ ì‹¬ë³¼ë§Œ ì‚¬ìš© (í…ŒìŠ¤íŠ¸ìš©)
    top_symbols = result_df.groupby('symbol').size().nlargest(max_symbols).index
    result_df = result_df[result_df['symbol'].isin(top_symbols)]
    
    # ê·¹ë‹¨ì  ìˆ˜ìµë¥  ì œê±° (Â±20% ì´ˆê³¼)
    result_df = result_df[
        (result_df['return'] >= -0.2) & (result_df['return'] <= 0.2)
    ]
    
    logger.info(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(result_df)} ê´€ì¸¡ì¹˜, {result_df['symbol'].nunique()} ì‹¬ë³¼")
    logger.info(f"ê¸°ê°„: {result_df['date'].min()} ~ {result_df['date'].max()}")
    logger.info(f"ì‹¬ë³¼: {sorted(result_df['symbol'].unique())}")
    
    return result_df


def test_out_of_sample_residuals():
    """ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ì”ì°¨ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*60)
    logger.info("ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ì”ì°¨ ê³„ì‚° í…ŒìŠ¤íŠ¸")
    logger.info("="*60)
    
    try:
        # ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„
        raw_df = load_processed_data()
        crypto_df = prepare_crypto_data(raw_df, max_symbols=6)
        
        # ë°ì´í„° ë¶„í•  (70% í•™ìŠµ, 30% í…ŒìŠ¤íŠ¸)
        unique_dates = sorted(crypto_df['date'].unique())
        split_idx = int(len(unique_dates) * 0.7)
        split_date = unique_dates[split_idx]
        
        train_data = crypto_df[crypto_df['date'] < split_date].copy()
        test_data = crypto_df[crypto_df['date'] >= split_date].copy()
        
        logger.info(f"í•™ìŠµ ë°ì´í„°: {len(train_data)} ê´€ì¸¡ì¹˜ ({train_data['date'].min()} ~ {train_data['date'].max()})")
        logger.info(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_data)} ê´€ì¸¡ì¹˜ ({test_data['date'].min()} ~ {test_data['date'].max()})")
        
        # ì”ì°¨ ê³„ì‚°ê¸° ì´ˆê¸°í™”
        config = ResidualCalculatorConfig(
            rolling_window_size=120,  # 4ê°œì›”
            min_observations=60,      # 2ê°œì›”
            refit_frequency=20,       # 20ì¼ë§ˆë‹¤ ì¬í•™ìŠµ
            quality_check_enabled=True,
            outlier_threshold=3.0
        )
        
        calculator = ResidualCalculator(config)
        
        # ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ì”ì°¨ ê³„ì‚°
        start_time = time.time()
        residuals_df, model = calculator.calculate_out_of_sample_residuals(
            train_data, test_data, n_factors=3
        )
        calculation_time = time.time() - start_time
        
        logger.info(f"ì”ì°¨ ê³„ì‚° ì™„ë£Œ (ì†Œìš”ì‹œê°„: {calculation_time:.2f}ì´ˆ)")
        logger.info(f"ì”ì°¨ ë°ì´í„° í˜•íƒœ: {residuals_df.shape}")
        logger.info(f"ì‹¬ë³¼ ìˆ˜: {residuals_df['symbol'].nunique()}")
        
        # ì”ì°¨ í†µê³„
        logger.info("\nì”ì°¨ ê¸°ë³¸ í†µê³„:")
        logger.info(f"  í‰ê· : {residuals_df['residual'].mean():.6f}")
        logger.info(f"  í‘œì¤€í¸ì°¨: {residuals_df['residual'].std():.6f}")
        logger.info(f"  ìµœì†Ÿê°’: {residuals_df['residual'].min():.6f}")
        logger.info(f"  ìµœëŒ“ê°’: {residuals_df['residual'].max():.6f}")
        
        # ì‹¬ë³¼ë³„ ì”ì°¨ í†µê³„
        logger.info("\nì‹¬ë³¼ë³„ ì”ì°¨ í†µê³„:")
        symbol_stats = residuals_df.groupby('symbol')['residual'].agg(['count', 'mean', 'std'])
        for symbol in symbol_stats.index:
            count = symbol_stats.loc[symbol, 'count']
            mean = symbol_stats.loc[symbol, 'mean']
            std = symbol_stats.loc[symbol, 'std']
            logger.info(f"  {symbol}: {count}ê°œ, í‰ê· ={mean:.6f}, í‘œì¤€í¸ì°¨={std:.6f}")
        
        return residuals_df, calculator
        
    except Exception as e:
        logger.error(f"ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ì”ì°¨ ê³„ì‚° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return None, None


def test_rolling_residuals():
    """ë¡¤ë§ ìœˆë„ìš° ì”ì°¨ ê³„ì‚° í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*60)
    logger.info("ë¡¤ë§ ìœˆë„ìš° ì”ì°¨ ê³„ì‚° í…ŒìŠ¤íŠ¸")
    logger.info("="*60)
    
    try:
        # ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„ (ì‘ì€ ë°ì´í„°ì…‹)
        raw_df = load_processed_data()
        crypto_df = prepare_crypto_data(raw_df, max_symbols=4)
        
        # ìµœê·¼ 6ê°œì›” ë°ì´í„°ë§Œ ì‚¬ìš© (ì„±ëŠ¥ìƒ ì´ìœ )
        unique_dates = sorted(crypto_df['date'].unique())
        if len(unique_dates) > 180:
            start_date = unique_dates[-180]
            crypto_df = crypto_df[crypto_df['date'] >= start_date]
        
        logger.info(f"ë¡¤ë§ í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(crypto_df)} ê´€ì¸¡ì¹˜")
        logger.info(f"ê¸°ê°„: {crypto_df['date'].min()} ~ {crypto_df['date'].max()}")
        
        # ì”ì°¨ ê³„ì‚°ê¸° ì´ˆê¸°í™” (ì‘ì€ ìœˆë„ìš°)
        config = ResidualCalculatorConfig(
            rolling_window_size=60,   # 2ê°œì›”
            min_observations=30,      # 1ê°œì›”
            refit_frequency=15,       # 15ì¼ë§ˆë‹¤ ì¬í•™ìŠµ
            quality_check_enabled=True
        )
        
        calculator = ResidualCalculator(config)
        
        # ë¡¤ë§ ì”ì°¨ ê³„ì‚°
        start_time = time.time()
        residuals_df = calculator.calculate_rolling_residuals(
            crypto_df, refit_models=True
        )
        calculation_time = time.time() - start_time
        
        logger.info(f"ë¡¤ë§ ì”ì°¨ ê³„ì‚° ì™„ë£Œ (ì†Œìš”ì‹œê°„: {calculation_time:.2f}ì´ˆ)")
        
        if len(residuals_df) > 0:
            logger.info(f"ì”ì°¨ ë°ì´í„° í˜•íƒœ: {residuals_df.shape}")
            logger.info(f"ì‹¬ë³¼ ìˆ˜: {residuals_df['symbol'].nunique()}")
            logger.info(f"ë‚ ì§œ ë²”ìœ„: {residuals_df['date'].min()} ~ {residuals_df['date'].max()}")
            
            # ì”ì°¨ í†µê³„
            logger.info("\në¡¤ë§ ì”ì°¨ ê¸°ë³¸ í†µê³„:")
            logger.info(f"  í‰ê· : {residuals_df['residual'].mean():.6f}")
            logger.info(f"  í‘œì¤€í¸ì°¨: {residuals_df['residual'].std():.6f}")
            
            return residuals_df, calculator
        else:
            logger.warning("ë¡¤ë§ ì”ì°¨ ê³„ì‚° ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤ (ë°ì´í„° ë¶€ì¡±)")
            return pd.DataFrame(), calculator
        
    except Exception as e:
        logger.error(f"ë¡¤ë§ ì”ì°¨ ê³„ì‚° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame(), None


def test_quality_metrics(residuals_df, calculator):
    """ì”ì°¨ í’ˆì§ˆ ì§€í‘œ í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*60)
    logger.info("ì”ì°¨ í’ˆì§ˆ ì§€í‘œ í…ŒìŠ¤íŠ¸")
    logger.info("="*60)
    
    if len(residuals_df) == 0:
        logger.warning("ì”ì°¨ ë°ì´í„°ê°€ ì—†ì–´ í’ˆì§ˆ ì§€í‘œë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {}
    
    try:
        # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
        start_time = time.time()
        quality_metrics = calculator.calculate_residual_quality_metrics(residuals_df)
        calculation_time = time.time() - start_time
        
        logger.info(f"í’ˆì§ˆ ì§€í‘œ ê³„ì‚° ì™„ë£Œ (ì†Œìš”ì‹œê°„: {calculation_time:.2f}ì´ˆ)")
        logger.info(f"ë¶„ì„ëœ ì‹¬ë³¼ ìˆ˜: {len(quality_metrics)}")
        
        # í’ˆì§ˆ ì§€í‘œ ì¶œë ¥
        for symbol, metrics in quality_metrics.items():
            logger.info(f"\n{symbol} í’ˆì§ˆ ì§€í‘œ:")
            logger.info(f"  ê´€ì¸¡ì¹˜ ìˆ˜: {metrics.n_observations}")
            logger.info(f"  í‰ê·  ì”ì°¨: {metrics.mean_residual:.6f}")
            logger.info(f"  í‘œì¤€í¸ì°¨: {metrics.std_residual:.6f}")
            logger.info(f"  ì™œë„: {metrics.skewness:.4f}")
            logger.info(f"  ì²¨ë„: {metrics.kurtosis:.4f}")
            logger.info(f"  ì •ìƒì„±: {'ì˜ˆ' if metrics.is_stationary else 'ì•„ë‹ˆì˜¤'} (p={metrics.adf_pvalue:.4f})")
            logger.info(f"  ìê¸°ìƒê´€(lag1): {metrics.autocorr_lag1:.4f}")
            logger.info(f"  ìê¸°ìƒê´€(lag5): {metrics.autocorr_lag5:.4f}")
            logger.info(f"  Jarque-Bera p-value: {metrics.jarque_bera_pvalue:.4f}")
            logger.info(f"  Ljung-Box p-value: {metrics.ljung_box_pvalue:.4f}")
            logger.info(f"  ì´ë¶„ì‚°ì„± ë¹„ìœ¨: {metrics.heteroskedasticity_test:.4f}")
        
        return quality_metrics
        
    except Exception as e:
        logger.error(f"í’ˆì§ˆ ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {}


def test_outlier_detection(residuals_df, calculator):
    """ì´ìƒì¹˜ íƒì§€ í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*60)
    logger.info("ì´ìƒì¹˜ íƒì§€ í…ŒìŠ¤íŠ¸")
    logger.info("="*60)
    
    if len(residuals_df) == 0:
        logger.warning("ì”ì°¨ ë°ì´í„°ê°€ ì—†ì–´ ì´ìƒì¹˜ íƒì§€ë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return pd.DataFrame()
    
    try:
        methods = ['zscore', 'iqr']
        
        for method in methods:
            logger.info(f"\n{method.upper()} ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€:")
            
            start_time = time.time()
            outliers_df = calculator.detect_residual_outliers(residuals_df, method=method)
            calculation_time = time.time() - start_time
            
            n_outliers = outliers_df['is_outlier'].sum()
            outlier_ratio = n_outliers / len(outliers_df) * 100
            
            logger.info(f"  ì†Œìš”ì‹œê°„: {calculation_time:.2f}ì´ˆ")
            logger.info(f"  ì „ì²´ ê´€ì¸¡ì¹˜: {len(outliers_df)}")
            logger.info(f"  ì´ìƒì¹˜ ìˆ˜: {n_outliers}")
            logger.info(f"  ì´ìƒì¹˜ ë¹„ìœ¨: {outlier_ratio:.2f}%")
            
            # ì‹¬ë³¼ë³„ ì´ìƒì¹˜ í†µê³„
            symbol_outliers = outliers_df.groupby('symbol')['is_outlier'].agg(['count', 'sum'])
            logger.info("  ì‹¬ë³¼ë³„ ì´ìƒì¹˜:")
            for symbol in symbol_outliers.index:
                total = symbol_outliers.loc[symbol, 'count']
                outliers = symbol_outliers.loc[symbol, 'sum']
                ratio = outliers / total * 100 if total > 0 else 0
                logger.info(f"    {symbol}: {outliers}/{total} ({ratio:.1f}%)")
        
        return outliers_df
        
    except Exception as e:
        logger.error(f"ì´ìƒì¹˜ íƒì§€ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return pd.DataFrame()


def test_residual_report(residuals_df, quality_metrics, calculator):
    """ì”ì°¨ ë¦¬í¬íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*60)
    logger.info("ì”ì°¨ ë¦¬í¬íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸")
    logger.info("="*60)
    
    if len(residuals_df) == 0:
        logger.warning("ì”ì°¨ ë°ì´í„°ê°€ ì—†ì–´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        return {}
    
    try:
        # ë¦¬í¬íŠ¸ ìƒì„±
        start_time = time.time()
        report = calculator.generate_residual_report(residuals_df, quality_metrics)
        calculation_time = time.time() - start_time
        
        logger.info(f"ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ (ì†Œìš”ì‹œê°„: {calculation_time:.2f}ì´ˆ)")
        
        # ë¦¬í¬íŠ¸ ë‚´ìš© ì¶œë ¥
        logger.info("\nğŸ“Š ì”ì°¨ ë¶„ì„ ë¦¬í¬íŠ¸:")
        logger.info(f"  ì „ì²´ ê´€ì¸¡ì¹˜ ìˆ˜: {report['total_observations']:,}")
        logger.info(f"  ë¶„ì„ ì‹¬ë³¼ ìˆ˜: {report['n_symbols']}")
        logger.info(f"  ë¶„ì„ ê¸°ê°„: {report['start_date']} ~ {report['end_date']}")
        logger.info(f"  í‰ê·  ìê¸°ìƒê´€(lag1): {report['mean_autocorr_lag1']:.4f}")
        logger.info(f"  í‰ê·  ìê¸°ìƒê´€(lag5): {report['mean_autocorr_lag5']:.4f}")
        logger.info(f"  ì •ìƒì„± ë¹„ìœ¨: {report['stationary_ratio']:.2%}")
        logger.info(f"  í‰ê·  Jarque-Bera p-value: {report['mean_jb_pvalue']:.4f}")
        logger.info(f"  ì–‘í˜¸í•œ ìê¸°ìƒê´€ ë¹„ìœ¨: {report['good_autocorr_ratio']:.2%}")
        logger.info(f"  ì´ìƒì¹˜ ë¹„ìœ¨: {report['outlier_ratio']:.2%}")
        logger.info(f"  ì „ì²´ í’ˆì§ˆ ì ìˆ˜: {report['quality_score']:.1f}/100")
        
        # í’ˆì§ˆ í‰ê°€
        if report['quality_score'] >= 80:
            quality_level = "ìš°ìˆ˜"
        elif report['quality_score'] >= 60:
            quality_level = "ì–‘í˜¸"
        elif report['quality_score'] >= 40:
            quality_level = "ë³´í†µ"
        else:
            quality_level = "ê°œì„  í•„ìš”"
        
        logger.info(f"  í’ˆì§ˆ í‰ê°€: {quality_level}")
        
        return report
        
    except Exception as e:
        logger.error(f"ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return {}


def test_data_persistence(residuals_df, calculator):
    """ë°ì´í„° ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*60)
    logger.info("ë°ì´í„° ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸")
    logger.info("="*60)
    
    if len(residuals_df) == 0:
        logger.warning("ì €ì¥í•  ì”ì°¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
        return False
    
    try:
        # ì„ì‹œ íŒŒì¼ ê²½ë¡œ
        temp_file = f"temp_residuals_{int(time.time())}.parquet"
        
        # ë°ì´í„° ì €ì¥
        start_time = time.time()
        calculator.save_residuals(residuals_df, temp_file)
        save_time = time.time() - start_time
        
        logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {save_time:.2f}ì´ˆ)")
        logger.info(f"íŒŒì¼ í¬ê¸°: {Path(temp_file).stat().st_size / 1024:.1f} KB")
        
        # ë°ì´í„° ë¡œë“œ
        start_time = time.time()
        loaded_df = calculator.load_residuals(temp_file)
        load_time = time.time() - start_time
        
        logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {load_time:.2f}ì´ˆ)")
        
        # ë°ì´í„° ì¼ì¹˜ì„± í™•ì¸
        if len(loaded_df) == len(residuals_df):
            logger.info("âœ… ë°ì´í„° í¬ê¸° ì¼ì¹˜")
        else:
            logger.error(f"âŒ ë°ì´í„° í¬ê¸° ë¶ˆì¼ì¹˜: {len(loaded_df)} vs {len(residuals_df)}")
            return False
        
        # ì»¬ëŸ¼ í™•ì¸
        if set(loaded_df.columns) == set(residuals_df.columns):
            logger.info("âœ… ì»¬ëŸ¼ êµ¬ì¡° ì¼ì¹˜")
        else:
            logger.error("âŒ ì»¬ëŸ¼ êµ¬ì¡° ë¶ˆì¼ì¹˜")
            return False
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        Path(temp_file).unlink()
        logger.info("ì„ì‹œ íŒŒì¼ ì‚­ì œ ì™„ë£Œ")
        
        return True
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False


def performance_benchmark():
    """ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
    logger.info("\n" + "="*60)
    logger.info("ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸")
    logger.info("="*60)
    
    try:
        # ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸°ë¡œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
        sizes = [2, 4, 6]  # ì‹¬ë³¼ ìˆ˜
        
        for n_symbols in sizes:
            logger.info(f"\n{n_symbols}ê°œ ì‹¬ë³¼ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸:")
            
            # ë°ì´í„° ì¤€ë¹„
            raw_df = load_processed_data()
            crypto_df = prepare_crypto_data(raw_df, max_symbols=n_symbols)
            
            # ìµœê·¼ 3ê°œì›” ë°ì´í„°ë§Œ ì‚¬ìš©
            unique_dates = sorted(crypto_df['date'].unique())
            if len(unique_dates) > 90:
                start_date = unique_dates[-90]
                crypto_df = crypto_df[crypto_df['date'] >= start_date]
            
            logger.info(f"  ë°ì´í„° í¬ê¸°: {len(crypto_df)} ê´€ì¸¡ì¹˜")
            
            # ë°ì´í„° ë¶„í• 
            split_idx = int(len(crypto_df) * 0.7)
            train_data = crypto_df.iloc[:split_idx]
            test_data = crypto_df.iloc[split_idx:]
            
            # ì”ì°¨ ê³„ì‚°ê¸°
            calculator = ResidualCalculator()
            
            # ì„±ëŠ¥ ì¸¡ì •
            start_time = time.time()
            residuals_df, model = calculator.calculate_out_of_sample_residuals(
                train_data, test_data, n_factors=2
            )
            total_time = time.time() - start_time
            
            # ê²°ê³¼ ì¶œë ¥
            throughput = len(test_data) / total_time if total_time > 0 else 0
            logger.info(f"  ì†Œìš”ì‹œê°„: {total_time:.2f}ì´ˆ")
            logger.info(f"  ì²˜ë¦¬ëŸ‰: {throughput:.1f} ê´€ì¸¡ì¹˜/ì´ˆ")
            logger.info(f"  ì”ì°¨ ìˆ˜: {len(residuals_df)}")
        
    except Exception as e:
        logger.error(f"ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    logger.info("ğŸš€ ì”ì°¨ ê³„ì‚°ê¸° í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    logger.info("=" * 80)
    
    # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì¶”ì 
    test_results = {}
    
    # 1. ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ì”ì°¨ ê³„ì‚° í…ŒìŠ¤íŠ¸
    residuals_df, calculator = test_out_of_sample_residuals()
    test_results['out_of_sample'] = residuals_df is not None and len(residuals_df) > 0
    
    # 2. ë¡¤ë§ ìœˆë„ìš° ì”ì°¨ ê³„ì‚° í…ŒìŠ¤íŠ¸
    rolling_residuals_df, rolling_calculator = test_rolling_residuals()
    test_results['rolling'] = rolling_residuals_df is not None and len(rolling_residuals_df) > 0
    
    # ì”ì°¨ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ í›„ì† í…ŒìŠ¤íŠ¸ ì§„í–‰
    if test_results['out_of_sample']:
        # 3. í’ˆì§ˆ ì§€í‘œ í…ŒìŠ¤íŠ¸
        quality_metrics = test_quality_metrics(residuals_df, calculator)
        test_results['quality_metrics'] = len(quality_metrics) > 0
        
        # 4. ì´ìƒì¹˜ íƒì§€ í…ŒìŠ¤íŠ¸
        outliers_df = test_outlier_detection(residuals_df, calculator)
        test_results['outlier_detection'] = len(outliers_df) > 0
        
        # 5. ë¦¬í¬íŠ¸ ìƒì„± í…ŒìŠ¤íŠ¸
        report = test_residual_report(residuals_df, quality_metrics, calculator)
        test_results['report_generation'] = len(report) > 0
        
        # 6. ë°ì´í„° ì €ì¥/ë¡œë“œ í…ŒìŠ¤íŠ¸
        test_results['data_persistence'] = test_data_persistence(residuals_df, calculator)
    else:
        logger.warning("ì•„ì›ƒì˜¤ë¸Œìƒ˜í”Œ ì”ì°¨ ë°ì´í„°ê°€ ì—†ì–´ í›„ì† í…ŒìŠ¤íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤")
        test_results.update({
            'quality_metrics': False,
            'outlier_detection': False,
            'report_generation': False,
            'data_persistence': False
        })
    
    # 7. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    performance_benchmark()
    test_results['performance_benchmark'] = True
    
    # ìµœì¢… ê²°ê³¼ ìš”ì•½
    logger.info("\n" + "="*80)
    logger.info("ğŸ¯ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info("="*80)
    
    passed_tests = sum(test_results.values())
    total_tests = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"  {test_name}: {status}")
    
    logger.info(f"\nğŸ“Š ì „ì²´ ê²°ê³¼: {passed_tests}/{total_tests} í…ŒìŠ¤íŠ¸ í†µê³¼")
    
    if passed_tests == total_tests:
        logger.info("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        logger.warning(f"âš ï¸  {total_tests - passed_tests}ê°œ í…ŒìŠ¤íŠ¸ê°€ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
    
    logger.info("\nâœ¨ ì”ì°¨ ê³„ì‚°ê¸° í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")


if __name__ == "__main__":
    main()