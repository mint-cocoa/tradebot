#!/usr/bin/env python3
"""
ì”ì°¨ ê³„ì‚° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ìŠ¤í¬ë¦½íŠ¸

ë‹¤ì–‘í•œ ë°ì´í„° í¬ê¸°ì™€ ì„¤ì •ì—ì„œ ì”ì°¨ ê³„ì‚° ì„±ëŠ¥ì„ ì¸¡ì •í•©ë‹ˆë‹¤.
"""

import sys
import os
import time
try:
    import psutil  # type: ignore
except Exception:  # pragma: no cover - optional dependency
    psutil = None
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import argparse
import json
from pathlib import Path
from typing import List, Tuple, Optional

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..'))

from crypto_dlsa_bot.ml.residual_calculator import ResidualCalculator, ResidualCalculatorConfig
from crypto_dlsa_bot.utils.logging import setup_logging, get_logger

setup_logging("INFO")
logger = get_logger(__name__)


def _compute_cross_sectional_ic(panel: pd.DataFrame) -> Tuple[float, float, int]:
    """
    ë‚ ì§œë³„ ë‹¨ë©´ ìƒê´€(Information Coefficient)ì„ ê³„ì‚°í•˜ê³  í‰ê· /í‘œì¤€í¸ì°¨/ìœ íš¨ì¼ìˆ˜ë¥¼ ë°˜í™˜.
    Pearson ìƒê´€ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    required = {'date', 'symbol', 'actual_ret', 'expected_ret'}
    if not required.issubset(panel.columns):
        return 0.0, 0.0, 0

    ics = []
    for d, g in panel.groupby('date'):
        g = g.dropna(subset=['actual_ret', 'expected_ret'])
        if g['symbol'].nunique() < 2:
            continue
        x = g['expected_ret'].astype(float)
        y = g['actual_ret'].astype(float)
        # ë°©ì–´ ë¡œì§: ìƒìˆ˜ì—´ ë°©ì§€
        if x.std(ddof=0) == 0 or y.std(ddof=0) == 0:
            continue
        ic = x.corr(y)  # Pearson
        if pd.notna(ic):
            ics.append(ic)

    if not ics:
        return 0.0, 0.0, 0
    arr = np.asarray(ics, dtype=float)
    return float(arr.mean()), float(arr.std(ddof=1) if len(arr) > 1 else 0.0), int(len(arr))


def _compute_oos_error_metrics(panel: pd.DataFrame) -> dict:
    """OOS ì˜ˆì¸¡ì˜¤ì°¨ ì§€í‘œ(MSE/MAE/R2_OS)ë¥¼ ê³„ì‚°."""
    required = {'actual_ret', 'expected_ret'}
    if not required.issubset(panel.columns) or panel.empty:
        return {'mse': None, 'mae': None, 'r2_os': None}

    df = panel.dropna(subset=['actual_ret', 'expected_ret']).copy()
    if df.empty:
        return {'mse': None, 'mae': None, 'r2_os': None}
    err = (df['actual_ret'] - df['expected_ret']).astype(float)
    mse = float(np.mean(np.square(err)))
    mae = float(np.mean(np.abs(err)))
    # R2_OS = 1 - SSE/SST (SSTëŠ” ì‹¤ì œê°’ì—ì„œ ì „ì²´ í‰ê· ì„ ëº€ ì œê³±í•©)
    sse = float(np.sum(np.square(err)))
    y = df['actual_ret'].astype(float)
    sst = float(np.sum(np.square(y - y.mean())))
    r2_os = 1.0 - sse / sst if sst > 0 else None
    return {'mse': mse, 'mae': mae, 'r2_os': r2_os}


def _backtest_long_short(panel: pd.DataFrame, q: float = 0.2) -> dict:
    """
    expected_retë¥¼ ì‹ í˜¸ë¡œ ë‚ ì§œë³„ ìƒìœ„ q, í•˜ìœ„ q í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ êµ¬ì„±í•´ ë¡±ìˆ ìˆ˜ìµë¥ ì„ ì‚°ì¶œ.
    Sharpe(ë¹„ì—°ìœ¨í™”), hit rate(ì–‘ì˜ ìˆ˜ìµë¥  ë¹„ìœ¨), ê¸°ê°„ ìˆ˜ë¥¼ ë°˜í™˜.
    """
    required = {'date', 'symbol', 'actual_ret', 'expected_ret'}
    if not required.issubset(panel.columns):
        return {'ls_mean': None, 'ls_std': None, 'ls_sharpe': None, 'hit_rate': None, 'n_days': 0}

    daily_ls = []
    for d, g in panel.groupby('date'):
        g = g.dropna(subset=['actual_ret', 'expected_ret'])
        if g['symbol'].nunique() < 3:
            continue
        k = max(1, int(len(g) * q))
        g = g.sort_values('expected_ret')
        bottom = g.head(k)
        top = g.tail(k)
        if bottom.empty or top.empty:
            continue
        # equal-weight
        long_ret = float(top['actual_ret'].mean())
        short_ret = float(bottom['actual_ret'].mean())
        daily_ls.append(long_ret - short_ret)

    if not daily_ls:
        return {'ls_mean': None, 'ls_std': None, 'ls_sharpe': None, 'hit_rate': None, 'n_days': 0}

    arr = np.asarray(daily_ls, dtype=float)
    ls_mean = float(arr.mean())
    ls_std = float(arr.std(ddof=1) if len(arr) > 1 else 0.0)
    ls_sharpe = float(ls_mean / ls_std) if ls_std > 0 else None
    hit_rate = float(np.mean(arr > 0))
    return {'ls_mean': ls_mean, 'ls_std': ls_std, 'ls_sharpe': ls_sharpe, 'hit_rate': hit_rate, 'n_days': int(len(arr))}


def _find_latest_parquet(base_dir: Path, timeframe: str) -> Optional[Path]:
    """ì§€ì •ëœ ì‹œê°„í”„ë ˆì„ì˜ ìµœì‹  ë©€í‹° ì‹¬ë³¼ Parquet íŒŒì¼ ê²½ë¡œë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    dir_path = base_dir / timeframe / "multi_symbol"
    if not dir_path.exists():
        return None
    files = list(dir_path.glob("*.parquet"))
    if not files:
        return None
    return max(files, key=lambda p: p.stat().st_mtime)


def load_real_ohlcv(
    symbols: List[str],
    timeframe: str,
    start_date: Optional[datetime] = None,
    end_date: Optional[datetime] = None,
    data_base_dir: str = "data/processed/ohlcv"
) -> pd.DataFrame:
    """
    ì²˜ë¦¬ëœ ì‹¤ì¸¡ ì°¨íŠ¸ ë°ì´í„°ë¥¼ Parquetì—ì„œ ë¡œë“œí•©ë‹ˆë‹¤.

    Args:
        symbols: ëŒ€ìƒ ì‹¬ë³¼ ëª©ë¡ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ë©´ ì „ì²´)
        timeframe: '1d', '4h', '1h' ë“±
        start_date: ì‹œì‘ì¼ (Noneì´ë©´ íŒŒì¼ ì „ì²´ ë²”ìœ„)
        end_date: ì¢…ë£Œì¼ (Noneì´ë©´ íŒŒì¼ ì „ì²´ ë²”ìœ„)
        data_base_dir: ì €ì¥ì†Œ ê¸°ë³¸ ê²½ë¡œ

    Returns:
        OHLCV DataFrame [timestamp, symbol, open, high, low, close, volume, ...]
    """
    base_dir = Path(data_base_dir)
    latest = _find_latest_parquet(base_dir, timeframe)
    if not latest or not latest.exists():
        raise FileNotFoundError(
            f"ì‹¤ì¸¡ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_dir}/{timeframe}/multi_symbol/*.parquet"
        )

    df = pd.read_parquet(latest)
    if df.empty:
        raise ValueError("ë¡œë“œëœ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")

    if 'timestamp' not in df.columns:
        raise ValueError("Parquet íŒŒì¼ì— 'timestamp' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤")

    df = df.copy()
    df['timestamp'] = pd.to_datetime(df['timestamp'])

    if symbols:
        df = df[df['symbol'].isin([s.upper() for s in symbols])]

    if start_date:
        df = df[df['timestamp'] >= pd.to_datetime(start_date)]
    if end_date:
        df = df[df['timestamp'] <= pd.to_datetime(end_date)]

    # í•„ìˆ˜ ì»¬ëŸ¼ ë³´ì •
    for col in ['open', 'high', 'low', 'close']:
        if col not in df.columns:
            raise ValueError(f"Parquetì— '{col}' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
    if 'volume' not in df.columns:
        df['volume'] = 0.0

    df = df.sort_values(['symbol', 'timestamp']).drop_duplicates(['symbol', 'timestamp'])
    return df


def prepare_returns_from_ohlcv(ohlcv: pd.DataFrame) -> pd.DataFrame:
    """OHLCVì—ì„œ ì¼ìë³„ ìˆ˜ìµë¥  íŒ¨ë„ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    if ohlcv.empty:
        return pd.DataFrame(columns=['symbol', 'date', 'return', 'close', 'volume'])

    df = ohlcv.copy()
    # ì¼ì ì •ê·œí™” (timeframeì´ ì¼ë´‰ì´ ì•„ë‹ˆì–´ë„ ì¼ ë‹¨ìœ„ë¡œ ì •ê·œí™”í•´ ì¼ë³„ íŒ¨ë„ êµ¬ì„±)
    df['date'] = pd.to_datetime(df['timestamp']).dt.normalize()
    df = df.sort_values(['symbol', 'date'])

    # ìˆ˜ìµë¥  ê³„ì‚°
    df['return'] = df.groupby('symbol')['close'].pct_change()
    df = df.dropna(subset=['return'])
    # ì´ìƒì¹˜ í•„í„° (ëª…ë°±í•œ ì˜¤ë¥˜ ì œê±°)
    df = df[df['return'].abs() < 1.0]

    # í•„ìš” ì»¬ëŸ¼ë§Œ ë°˜í™˜
    return df[['symbol', 'date', 'return', 'close', 'volume']]


def create_characteristics_data(returns_data: pd.DataFrame) -> pd.DataFrame:
    """
    íŠ¹ì„± ë°ì´í„° ìƒì„±
    
    Args:
        returns_data: ìˆ˜ìµë¥  ë°ì´í„°
        
    Returns:
        íŠ¹ì„± ë°ì´í„°
    """
    logger.info("íŠ¹ì„± ë°ì´í„° ìƒì„± ì¤‘...")
    
    characteristics = returns_data[['symbol', 'date', 'close', 'volume']].copy()
    
    # ê¸°ë³¸ íŠ¹ì„±ë“¤ (market_capì´ ì—†ëŠ” ì‹¤ì¸¡ ë°ì´í„° ê³ ë ¤)
    if 'market_cap' not in returns_data.columns:
        market_cap = returns_data['close'] * returns_data['volume']
    else:
        market_cap = returns_data['market_cap']
    characteristics['log_market_cap'] = np.log(market_cap.replace({0: np.nan}).fillna(0) + 1e-8)
    characteristics['log_volume'] = np.log(returns_data['volume'].replace({0: np.nan}).fillna(0) + 1e-8)
    
    # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
    returns_pivot = returns_data.pivot(index='date', columns='symbol', values='return')
    
    # ëª¨ë©˜í…€ (ì—¬ëŸ¬ ê¸°ê°„)
    for window in [5, 21, 63]:
        momentum = returns_pivot.rolling(window).sum().stack().reset_index()
        momentum.columns = ['date', 'symbol', f'momentum_{window}d']
        characteristics = characteristics.merge(momentum, on=['symbol', 'date'], how='left')
    
    # ë³€ë™ì„± (ì—¬ëŸ¬ ê¸°ê°„)
    for window in [5, 21, 63]:
        volatility = returns_pivot.rolling(window).std().stack().reset_index()
        volatility.columns = ['date', 'symbol', f'volatility_{window}d']
        characteristics = characteristics.merge(volatility, on=['symbol', 'date'], how='left')
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    characteristics = characteristics.drop(columns=['close'], errors='ignore')
    characteristics = characteristics.fillna(0)
    
    logger.info(f"íŠ¹ì„± ë°ì´í„° ìƒì„± ì™„ë£Œ: {characteristics.shape}")
    return characteristics


def run_single_benchmark(
    method: str,
    returns_data: pd.DataFrame,
    characteristics_data: Optional[pd.DataFrame],
    config: ResidualCalculatorConfig
) -> dict:
    """
    ë‹¨ì¼ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    
    Args:
        method: ê³„ì‚° ë°©ë²•
        returns_data: ìˆ˜ìµë¥  ë°ì´í„°
        characteristics_data: íŠ¹ì„± ë°ì´í„°
        config: ì„¤ì •
        
    Returns:
        ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    """
    logger.info(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰: {method}")

    calculator = ResidualCalculator(config)

    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì‹œì‘ (psutilì´ ì—†ìœ¼ë©´ 0ìœ¼ë¡œ ëŒ€ì²´)
    process = psutil.Process() if psutil else None
    memory_before = (process.memory_info().rss / 1024 / 1024) if process else 0.0  # MB

    # ì‹œê°„ ì¸¡ì • ì‹œì‘
    start_time = time.time()

    try:
        if method == 'standard':
            result = calculator.calculate_rolling_residuals(
                returns_data, save_models=False
            )
        elif method == 'optimized':
            result = calculator.calculate_rolling_residuals_optimized(
                returns_data, characteristics_data, save_models=False
            )
        elif method == 'parallel':
            result = calculator.calculate_residuals_parallel(
                returns_data, characteristics_data, n_jobs=2
            )
        elif method == 'streaming':
            result = calculator.calculate_residuals_streaming(
                returns_data, characteristics_data, batch_size=100
            )
        else:
            raise ValueError(f"ì•Œ ìˆ˜ ì—†ëŠ” ë°©ë²•: {method}")

        # ì‹œê°„ ì¸¡ì • ì¢…ë£Œ
        end_time = time.time()
        execution_time = end_time - start_time

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¸¡ì • ì¢…ë£Œ
        memory_after = (process.memory_info().rss / 1024 / 1024) if process else memory_before  # MB
        memory_used = (memory_after - memory_before) if process else 0.0

        # ëª¨ë¸ í’ˆì§ˆ ë° ë°±í…ŒìŠ¤íŠ¸ í‰ê°€ (ì‹¤ë°ì´í„° ê¸°ì¤€)
        model_metrics = {}
        backtest_metrics = {}
        if len(result) > 0:
            quality_metrics = calculator.calculate_residual_quality_metrics(result)
            quality_score = calculator._calculate_overall_quality_score(quality_metrics)
            # ì”ì°¨ í†µê³„
            residual_stats = calculator.get_residual_statistics(result)
            avg_autocorr = np.mean([
                abs(stats['autocorr_lag1']) for stats in residual_stats.values()
                if not np.isnan(stats['autocorr_lag1'])
            ]) if residual_stats else 0
            # ì˜ˆì¸¡ í’ˆì§ˆ (expected_ret vs actual_ret)
            ic_mean, ic_std, ic_days = _compute_cross_sectional_ic(result)
            oos_errors = _compute_oos_error_metrics(result)
            model_metrics = {
                'ic_mean': ic_mean,
                'ic_std': ic_std,
                'ic_days': ic_days,
                **oos_errors,
            }
            # ë‹¨ìˆœ ë¡±ìˆ ë°±í…ŒìŠ¤íŠ¸
            backtest_metrics = _backtest_long_short(result, q=0.2)
        else:
            quality_score = 0
            avg_autocorr = 0

        return {
            'method': method,
            'execution_time_seconds': execution_time,
            'memory_used_mb': memory_used,
            'output_rows': len(result),
            'throughput_rows_per_second': len(result) / execution_time if execution_time > 0 else 0,
            'quality_score': quality_score,
            'avg_autocorr': avg_autocorr,
            'success': True,
            'error': None
            , 'model_metrics': model_metrics
            , 'backtest': backtest_metrics
        }

    except Exception as e:
        logger.error(f"{method} ì‹¤íŒ¨: {e}")
        return {
            'method': method,
            'execution_time_seconds': 0,
            'memory_used_mb': 0,
            'output_rows': 0,
            'throughput_rows_per_second': 0,
            'quality_score': 0,
            'avg_autocorr': 0,
            'success': False,
            'error': str(e)
        }


def run_scalability_benchmark(
    methods: list,
    base_returns: pd.DataFrame,
    base_characteristics: Optional[pd.DataFrame] = None,
    max_symbols: int = 50,
    max_days: int = 500,
):
    """
    í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    
    Args:
        methods: í…ŒìŠ¤íŠ¸í•  ë°©ë²•ë“¤
        max_symbols: ìµœëŒ€ ì‹¬ë³¼ ìˆ˜
        max_days: ìµœëŒ€ ì¼ìˆ˜
    """
    logger.info("í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ë²”ìœ„ ë‚´ì—ì„œ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ êµ¬ì„±
    available_symbols = base_returns['symbol'].nunique()
    available_days = base_returns['date'].nunique()
    target_cases = [
        (min(10, available_symbols), min(100, available_days)),
        (min(20, available_symbols), min(200, available_days)),
        (min(30, available_symbols), min(300, available_days)),
    ]
    if max_symbols >= 50 and max_days >= 500 and available_symbols >= 50 and available_days >= 500:
        target_cases.append((50, 500))
    
    results = []
    
    config = ResidualCalculatorConfig(
        rolling_window_size=60,
        min_observations=20,
        refit_frequency=10
    )
    
    # ì‹¬ë³¼ì€ ê±°ë˜ëŒ€ê¸ˆ(= close*volume) í•©ê³„ ìƒìœ„ë¡œ ì„ íƒ, ë‚ ì§œëŠ” ìµœì‹ ë¶€í„° ìµœê·¼ n_daysë§Œ ì‚¬ìš©
    symbol_liquidity = (
        base_returns.assign(liq=base_returns['close'] * base_returns['volume'])
        .groupby('symbol')['liq']
        .sum()
        .sort_values(ascending=False)
    )

    sorted_dates = sorted(base_returns['date'].unique())

    for n_symbols, n_days in target_cases:
        if n_symbols < 2 or n_days < 10:
            continue

        logger.info(f"í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤: {n_symbols}ê°œ ì‹¬ë³¼, {n_days}ì¼")

        top_symbols = list(symbol_liquidity.index[:n_symbols])
        sel_dates = sorted_dates[-n_days:]

        returns_data = base_returns[
            base_returns['symbol'].isin(top_symbols) & base_returns['date'].isin(sel_dates)
        ].copy()
        characteristics_data = None
        if base_characteristics is not None and not base_characteristics.empty:
            characteristics_data = base_characteristics[
                base_characteristics['symbol'].isin(top_symbols) & base_characteristics['date'].isin(sel_dates)
            ].copy()
        
        case_results = {
            'n_symbols': n_symbols,
            'n_days': n_days,
            'total_observations': len(returns_data),
            'methods': {}
        }
        
        # ê° ë°©ë²•ë³„ í…ŒìŠ¤íŠ¸
        for method in methods:
            try:
                result = run_single_benchmark(method, returns_data, characteristics_data, config)
                case_results['methods'][method] = result
                logger.info(
                    f"  {method}: {result['execution_time_seconds']:.2f}ì´ˆ, "
                    f"{result['throughput_rows_per_second']:.1f} í–‰/ì´ˆ"
                )
            except Exception as e:
                logger.error(f"  {method} ì‹¤íŒ¨: {e}")
                case_results['methods'][method] = {'success': False, 'error': str(e)}

        results.append(case_results)
    
    return results


def run_configuration_benchmark(base_data: pd.DataFrame, base_characteristics: Optional[pd.DataFrame]):
    """
    ì„¤ì •ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
    
    Args:
        base_data: ê¸°ë³¸ ë°ì´í„°
        base_characteristics: ê¸°ë³¸ íŠ¹ì„± ë°ì´í„°
    """
    logger.info("ì„¤ì •ë³„ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸í•  ì„¤ì •ë“¤
    configs = [
        ResidualCalculatorConfig(rolling_window_size=30, min_observations=10, refit_frequency=5),
        ResidualCalculatorConfig(rolling_window_size=60, min_observations=20, refit_frequency=10),
        ResidualCalculatorConfig(rolling_window_size=120, min_observations=40, refit_frequency=20),
        ResidualCalculatorConfig(rolling_window_size=252, min_observations=60, refit_frequency=21),
    ]
    
    results = []
    
    for i, config in enumerate(configs):
        logger.info(
            f"ì„¤ì • {i+1}: ìœˆë„ìš°={config.rolling_window_size}, "
            f"ìµœì†Œê´€ì¸¡={config.min_observations}, ì¬í•™ìŠµ={config.refit_frequency}"
        )

        result = run_single_benchmark('optimized', base_data, base_characteristics, config)
        result['config'] = {
            'rolling_window_size': config.rolling_window_size,
            'min_observations': config.min_observations,
            'refit_frequency': config.refit_frequency
        }

        results.append(result)

        logger.info(
            f"  ê²°ê³¼: {result['execution_time_seconds']:.2f}ì´ˆ, "
            f"í’ˆì§ˆì ìˆ˜={result['quality_score']:.1f}"
        )
    
    return results


def save_benchmark_results(results: dict, output_file: str):
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥
    
    Args:
        results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
        output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    """
    # ê²°ê³¼ì— ë©”íƒ€ë°ì´í„° ì¶”ê°€
    cpu_count = psutil.cpu_count() if psutil else None
    mem_total_gb = (psutil.virtual_memory().total / (1024**3)) if psutil else None
    results['metadata'] = {
        'timestamp': datetime.now().isoformat(),
        'system_info': {
            'cpu_count': cpu_count,
            'memory_gb': mem_total_gb,
            'python_version': sys.version,
            'platform': sys.platform
        }
    }
    
    # JSONìœ¼ë¡œ ì €ì¥
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    logger.info(f"ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ì €ì¥: {output_file}")


def print_summary(results: dict):
    """
    ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥
    
    Args:
        results: ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼
    """
    print("\n" + "="*80)
    print("ì”ì°¨ ê³„ì‚° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ ìš”ì•½")
    print("="*80)
    
    if 'scalability' in results:
        print("\nğŸ“Š í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬:")
        for case in results['scalability']:
            print(f"\n  ë°ì´í„° í¬ê¸°: {case['n_symbols']}ê°œ ì‹¬ë³¼ Ã— {case['n_days']}ì¼ "
                  f"({case['total_observations']:,} ê´€ì¸¡ì¹˜)")
            
            for method, result in case['methods'].items():
                if result['success']:
                    print(f"    {method:12}: {result['execution_time_seconds']:6.2f}ì´ˆ "
                          f"({result['throughput_rows_per_second']:8.1f} í–‰/ì´ˆ)")
                    # ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
                    mm = result.get('model_metrics') or {}
                    bt = result.get('backtest') or {}
                    ic_mean = mm.get('ic_mean')
                    ic_std = mm.get('ic_std')
                    r2_os = mm.get('r2_os')
                    mse = mm.get('mse')
                    mae = mm.get('mae')
                    ls_sharpe = bt.get('ls_sharpe')
                    hit_rate = bt.get('hit_rate')
                    # ê°€ë…ì„± ìˆëŠ” ì¶œë ¥ (None í—ˆìš©)
                    def _fmt(x, fmt):
                        return (fmt % x) if isinstance(x, (int, float)) and x is not None else str(x)
                    print(
                        f"      Â· IC(mean/std)={_fmt(ic_mean, '%.3f')}/{_fmt(ic_std, '%.3f')}  "
                        f"R2_OS={_fmt(r2_os, '%.3f')}  MSE/MAE={_fmt(mse, '%.6f')}/{_fmt(mae, '%.6f')}  "
                        f"LS Sharpe={_fmt(ls_sharpe, '%.2f')}  Hit%={_fmt(hit_rate, '%.1f')}"
                    )
                else:
                    print(f"    {method:12}: ì‹¤íŒ¨ - {result.get('error', 'Unknown error')}")
    
    if 'configuration' in results:
        print("\nâš™ï¸  ì„¤ì •ë³„ ì„±ëŠ¥:")
        for result in results['configuration']:
            if result['success']:
                config = result['config']
                print(f"    ìœˆë„ìš°={config['rolling_window_size']:3d}, "
                      f"ìµœì†Œ={config['min_observations']:2d}, "
                      f"ì¬í•™ìŠµ={config['refit_frequency']:2d}: "
                      f"{result['execution_time_seconds']:6.2f}ì´ˆ "
                      f"(í’ˆì§ˆ={result['quality_score']:5.1f})")
                # ëª¨ë¸ ì„±ëŠ¥ ì§€í‘œ ìš”ì•½
                mm = result.get('model_metrics') or {}
                bt = result.get('backtest') or {}
                def _fmt(x, fmt):
                    return (fmt % x) if isinstance(x, (int, float)) and x is not None else str(x)
                print(
                    f"      Â· IC(mean/std)={_fmt(mm.get('ic_mean'), '%.3f')}/{_fmt(mm.get('ic_std'), '%.3f')}  "
                    f"R2_OS={_fmt(mm.get('r2_os'), '%.3f')}  MSE/MAE={_fmt(mm.get('mse'), '%.6f')}/{_fmt(mm.get('mae'), '%.6f')}  "
                    f"LS Sharpe={_fmt(bt.get('ls_sharpe'), '%.2f')}  Hit%={_fmt(bt.get('hit_rate'), '%.1f')}"
                )
    
    print("\n" + "="*80)


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description='ì”ì°¨ ê³„ì‚° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ (ì‹¤ì¸¡ ì°¨íŠ¸ ë°ì´í„° ê¸°ë°˜)')
    parser.add_argument('--methods', nargs='+', 
                       choices=['standard', 'optimized', 'parallel', 'streaming'],
                       default=['optimized', 'streaming'],
                       help='í…ŒìŠ¤íŠ¸í•  ë°©ë²•ë“¤')
    parser.add_argument('--max-symbols', type=int, default=30,
                       help='í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì‹œ ìµœëŒ€ ì‹¬ë³¼ ìˆ˜')
    parser.add_argument('--max-days', type=int, default=300,
                       help='í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ì‹œ ìµœëŒ€ ì¼ìˆ˜')
    parser.add_argument('--output', type=str, default='benchmark_results.json',
                       help='ê²°ê³¼ ì¶œë ¥ íŒŒì¼')
    parser.add_argument('--skip-scalability', action='store_true',
                       help='í™•ì¥ì„± í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--skip-config', action='store_true',
                       help='ì„¤ì • í…ŒìŠ¤íŠ¸ ê±´ë„ˆë›°ê¸°')
    parser.add_argument('--symbols', type=str, default='BTCUSDT,ETHUSDT,BNBUSDT',
                       help='ë²¤ì¹˜ë§ˆí¬ ëŒ€ìƒ ì‹¬ë³¼ (ì½¤ë§ˆë¡œ êµ¬ë¶„)')
    parser.add_argument('--timeframe', type=str, default='1d',
                       help='ë°ì´í„° ì‹œê°„ í”„ë ˆì„ (ì˜ˆ: 1d, 4h, 1h)')
    parser.add_argument('--start-date', type=str, default=None,
                       help='ì‹œì‘ì¼ YYYY-MM-DD (ë¯¸ì§€ì • ì‹œ ì „ì²´)')
    parser.add_argument('--end-date', type=str, default=None,
                       help='ì¢…ë£Œì¼ YYYY-MM-DD (ë¯¸ì§€ì • ì‹œ ì „ì²´)')
    parser.add_argument('--data-dir', type=str, default='data/processed/ohlcv',
                       help='ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì €ì¥ëœ ê¸°ë³¸ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    logger.info("ì”ì°¨ ê³„ì‚° ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹œì‘ (ì‹¤ì¸¡ ë°ì´í„°)")
    logger.info(f"í…ŒìŠ¤íŠ¸ ë°©ë²•: {args.methods}")
    logger.info(f"í™•ì¥ì„± ìƒí•œ: {args.max_symbols}ê°œ ì‹¬ë³¼, {args.max_days}ì¼")
    logger.info(f"ëŒ€ìƒ ì‹¬ë³¼: {args.symbols}")
    logger.info(f"ì‹œê°„í”„ë ˆì„: {args.timeframe}")
    
    all_results = {}
    
    try:
        # ì‹¤ì¸¡ ë°ì´í„° ë¡œë“œ
        symbols = [s.strip().upper() for s in (args.symbols or '').split(',') if s.strip()]
        start_dt = datetime.strptime(args.start_date, "%Y-%m-%d") if args.start_date else None
        end_dt = datetime.strptime(args.end_date, "%Y-%m-%d") if args.end_date else None

        ohlcv = load_real_ohlcv(symbols, args.timeframe, start_dt, end_dt, data_base_dir=args.data_dir)
        logger.info(f"ë¡œë”© ì™„ë£Œ: {len(ohlcv):,} ë ˆì½”ë“œ, ì‹¬ë³¼ {ohlcv['symbol'].nunique()}ê°œ, ë²”ìœ„ {ohlcv['timestamp'].min()} ~ {ohlcv['timestamp'].max()}")

        returns_panel = prepare_returns_from_ohlcv(ohlcv)
        characteristics_df = create_characteristics_data(returns_panel)

        # í™•ì¥ì„± ë²¤ì¹˜ë§ˆí¬
        if not args.skip_scalability:
            scalability_results = run_scalability_benchmark(
                args.methods,
                returns_panel,
                characteristics_df,
                max_symbols=args.max_symbols,
                max_days=args.max_days,
            )
            all_results['scalability'] = scalability_results

        # ì„¤ì •ë³„ ë²¤ì¹˜ë§ˆí¬ (ë™ì¼ ì‹¤ì¸¡ ë°ì´í„° ê¸°ë°˜)
        if not args.skip_config:
            config_results = run_configuration_benchmark(returns_panel, characteristics_df)
            all_results['configuration'] = config_results
        
        # ê²°ê³¼ ì €ì¥
        save_benchmark_results(all_results, args.output)
        
        # ìš”ì•½ ì¶œë ¥
        print_summary(all_results)
        
        logger.info("ë²¤ì¹˜ë§ˆí¬ ì™„ë£Œ")
        
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        raise


if __name__ == "__main__":
    main()